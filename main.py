import aiohttp
import asyncio
import time
import re
import logging
from lxml import etree
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult,MessageChain
from astrbot.api.star import Context, Star, register
from astrbot.api import AstrBotConfig
import astrbot.api.message_components as Comp

from .data_handler import DataHandler
from .pic_handler import RssImageHandler
from .rss import RSSItem
from typing import List


@register(
    "astrbot_plugin_rss",
    "Xiaobailoves",
    "RSSè®¢é˜…æ’ä»¶",
    "1.2.0",
    "https://github.com/xiaobailoves/astrbot_plugin_rss",
)
class RssPlugin(Star):
    # å¢åŠ ä¸€ä¸ªç±»å˜é‡ï¼Œç”¨äºå­˜å‚¨è°ƒåº¦å™¨å®ä¾‹ï¼Œé˜²æ­¢çƒ­é‡è½½æ—¶äº§ç”Ÿå¤šä¸ªå®šæ—¶å™¨æ³„æ¼ï¼ˆå¹½çµä»»åŠ¡ï¼‰
    _shared_scheduler = None

    def __init__(self, context: Context, config: AstrBotConfig) -> None:
        super().__init__(context)

        self.logger = logging.getLogger("astrbot")
        self.context = context
        self.config = config
        self.data_handler = DataHandler()

        # æå–schemeæ–‡ä»¶ä¸­çš„é…ç½®
        self.title_max_length = config.get("title_max_length")
        self.description_max_length = config.get("description_max_length")
        self.max_items_per_poll = config.get("max_items_per_poll")
        self.t2i = config.get("t2i")
        self.is_hide_url = config.get("is_hide_url")
        self.is_read_pic= config.get("pic_config").get("is_read_pic")
        self.is_adjust_pic= config.get("pic_config").get("is_adjust_pic")
        self.max_pic_item = config.get("pic_config").get("max_pic_item")
        self.is_compose = config.get("compose")
        self.proxy = config.get("proxy", None) # æ–°å¢ä»£ç†é…ç½®

        # ä¼ å…¥ä»£ç†é…ç½®ç»™å›¾ç‰‡å¤„ç†å™¨ï¼Œè¿™é‡Œä¿ç•™ä»£ç†
        self.pic_handler = RssImageHandler(self.is_adjust_pic, proxy=self.proxy)
        
        # --- ä¿®å¤å®šæ—¶ä»»åŠ¡é‡å¤è§¦å‘ï¼ˆæ³„æ¼ï¼‰çš„ Bug ---
        if RssPlugin._shared_scheduler is not None and RssPlugin._shared_scheduler.running:
            try:
                RssPlugin._shared_scheduler.shutdown(wait=False)
            except Exception as e:
                self.logger.warning(f"å…³é—­æ—§è°ƒåº¦å™¨æ—¶å‡ºç°å¼‚å¸¸: {e}")
        
        # åˆ›å»ºæ–°çš„è°ƒåº¦å™¨å¹¶ä¿å­˜åˆ°ç±»å˜é‡ä¸­
        RssPlugin._shared_scheduler = AsyncIOScheduler()
        self.scheduler = RssPlugin._shared_scheduler
        self.scheduler.start()
        # ---------------------------------------

        self._fresh_asyncIOScheduler()

    def parse_cron_expr(self, cron_expr: str):
        fields = cron_expr.split() 
        return {
            "minute": fields[0],
            "hour": fields[1],
            "day": fields[2],
            "month": fields[3],
            "day_of_week": fields[4],
        }

    async def parse_channel_info(self, url):
        headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        connector = aiohttp.TCPConnector(ssl=False)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        try:
            # ã€ä¿®æ”¹ç‚¹ 1ã€‘ï¼šå°† trust_env æ”¹ä¸º Falseï¼Œç¡®ä¿æ‹‰å– RSS æ—¶ä¸èµ°ç³»ç»Ÿçº§ä»£ç†
            async with aiohttp.ClientSession(trust_env=False,
                                        connector=connector,
                                        timeout=timeout,
                                        headers=headers
                                        ) as session:
                # ã€ä¿®æ”¹ç‚¹ 2ã€‘ï¼šç§»é™¤ proxy=self.proxyï¼Œç¡®ä¿æ‹‰å– RSS çº¯ç›´è¿
                async with session.get(url) as resp: 
                    if resp.status != 200:
                        self.logger.error(f"rss: æ— æ³•æ­£å¸¸æ‰“å¼€ç«™ç‚¹ {url}")
                        return None
                    text = await resp.read()
                    return text
        except asyncio.TimeoutError:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} è¶…æ—¶")
            return None
        except aiohttp.ClientError as e:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} ç½‘ç»œé”™è¯¯: {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
            return None

    async def cron_task_callback(self, url: str, user: str):
        """å®šæ—¶ä»»åŠ¡å›è°ƒ"""

        if url not in self.data_handler.data:
            return
        if user not in self.data_handler.data[url]["subscribers"]:
            return

        self.logger.info(f"RSS å®šæ—¶ä»»åŠ¡è§¦å‘: {url} - {user}")
        last_update = self.data_handler.data[url]["subscribers"][user]["last_update"]
        latest_link = self.data_handler.data[url]["subscribers"][user]["latest_link"]
        max_items_per_poll = self.max_items_per_poll
        # æ‹‰å– RSS
        rss_items = await self.poll_rss(
            url,
            num=max_items_per_poll,
            after_timestamp=last_update,
            after_link=latest_link,
        )
        if not rss_items:
            self.logger.info(f"RSS å®šæ—¶ä»»åŠ¡ {url} æ— æ¶ˆæ¯æ›´æ–° - {user}")
            return
            
        max_ts = last_update

        user_parts = user.split(":")
        platform_name = user_parts[0] if len(user_parts) > 0 else ""
        message_type = user_parts[1] if len(user_parts) > 1 else ""
        session_id = user_parts[2] if len(user_parts) > 2 else ""

        # åˆ†å¹³å°å¤„ç†æ¶ˆæ¯
        if platform_name == "aiocqhttp" and self.is_compose:
            nodes = []
            for item in rss_items:
                comps = await self._get_chain_components(item)
                node = Comp.Node(
                            uin=0,
                            name="Astrbot",
                            content=comps
                        )
                nodes.append(node)
                max_ts = max(max_ts, item.pubDate_timestamp)

            # åˆå¹¶æ¶ˆæ¯å‘é€
            if len(nodes) > 0:
                msc = MessageChain(
                    chain=nodes,
                    use_t2i_= self.t2i
                )
                await self.context.send_message(user, msc)
        else:
            # æ¯ä¸ªæ¶ˆæ¯å•ç‹¬å‘é€
            for item in rss_items:
                comps = await self._get_chain_components(item)
                msc = MessageChain(
                    chain=comps,
                    use_t2i_= self.t2i
                )
                await self.context.send_message(user, msc)
                max_ts = max(max_ts, item.pubDate_timestamp)

        # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
        self.data_handler.data[url]["subscribers"][user]["last_update"] = max_ts
        self.data_handler.data[url]["subscribers"][user]["latest_link"] = rss_items[-1].link
        self.data_handler.save_data()
        self.logger.info(f"RSS å®šæ—¶ä»»åŠ¡ {url} æ¨é€æˆåŠŸ - {user}")


    async def poll_rss(
        self,
        url: str,
        num: int = -1,
        after_timestamp: int = 0,
        after_link: str = "",
    ) -> List[RSSItem]:
        """ä»ç«™ç‚¹æ‹‰å–RSSä¿¡æ¯"""
        text = await self.parse_channel_info(url)
        if text is None:
            self.logger.error(f"rss: æ— æ³•è§£æç«™ç‚¹ {url} çš„RSSä¿¡æ¯")
            return []
        root = etree.fromstring(text)
        items = root.xpath("//item")

        cnt = 0
        rss_items = []

        for item in items:
            try:
                chan_title = (
                    self.data_handler.data[url]["info"]["title"]
                    if url in self.data_handler.data
                    else "æœªçŸ¥é¢‘é“"
                )

                raw_title = item.xpath("title")
                title = raw_title[0].text if (raw_title and raw_title[0].text) else "æ— æ ‡é¢˜"
                
                if len(title) > self.title_max_length:
                    title = title[: self.title_max_length] + "..."

                link_nodes = item.xpath("link")
                link = link_nodes[0].text if (link_nodes and link_nodes[0].text) else ""
                if link and not re.match(r"^https?://", link):
                    link = self.data_handler.get_root_url(url) + link
                
                raw_desc = item.xpath("description")
                description = raw_desc[0].text if (raw_desc and raw_desc[0].text) else ""

                pic_url_list = self.data_handler.strip_html_pic(description)
                description = self.data_handler.strip_html(description)

                if len(description) > self.description_max_length:
                    description = (
                        description[: self.description_max_length] + "..."
                    )

                pub_date_nodes = item.xpath("pubDate")
                if pub_date_nodes:
                    pub_date = item.xpath("pubDate")[0].text
                    try:
                        pub_date_parsed = time.strptime(
                            pub_date.replace("GMT", "+0000"),
                            "%a, %d %b %Y %H:%M:%S %z",
                        )
                        pub_date_timestamp = int(time.mktime(pub_date_parsed))
                    except:
                        pub_date_timestamp = 0

                    if pub_date_timestamp > after_timestamp:
                        rss_items.append(
                            RSSItem(
                                chan_title,
                                title,
                                link,
                                description,
                                pub_date,
                                pub_date_timestamp,
                                pic_url_list
                            )
                        )
                        cnt += 1
                else:
                    if link != after_link:
                        if not after_link and len(rss_items) >= 1:
                            continue
                        rss_items.append(
                            RSSItem(chan_title, title, link, description, "", 0, pic_url_list)
                        )
                        cnt += 1
                if num != -1 and cnt >= num:
                    break

            except Exception as e:
                self.logger.error(f"rss: è§£æRssæ¡ç›® {url} å¤±è´¥: {str(e)}")
                continue
        
        rss_items.reverse()
        return rss_items

    def parse_rss_url(self, url: str) -> str:
        """è§£æRSS URLï¼Œç¡®ä¿ä»¥httpæˆ–httpså¼€å¤´"""
        if not re.match(r"^https?://", url):
            if not url.startswith("/"):
                url = "/" + url
            url = "https://" + url
        return url

    def _fresh_asyncIOScheduler(self):
        """åˆ·æ–°å®šæ—¶ä»»åŠ¡"""
        self.logger.info("åˆ·æ–°å®šæ—¶ä»»åŠ¡")
        self.scheduler.remove_all_jobs()

        for url, info in self.data_handler.data.items():
            if url in ["rsshub_endpoints", "settings"]:
                continue
            for user, sub_info in info["subscribers"].items():
                job_id = f"{url}_{user}" 
                self.scheduler.add_job(
                    self.cron_task_callback,
                    "cron",
                    **self.parse_cron_expr(sub_info["cron_expr"]),
                    args=[url, user],
                    id=job_id,
                    replace_existing=True 
                )

    async def _add_url(self, url: str, cron_expr: str, message: AstrMessageEvent):
        """å†…éƒ¨æ–¹æ³•ï¼šæ·»åŠ URLè®¢é˜…çš„å…±ç”¨é€»è¾‘"""
        user = message.unified_msg_origin
        if url in self.data_handler.data:
            latest_item = await self.poll_rss(url)
            last_update = latest_item[-1].pubDate_timestamp if latest_item else int(time.time())
            latest_link = latest_item[-1].link if latest_item else ""
            
            self.data_handler.data[url]["subscribers"][user] = {
                "cron_expr": cron_expr,
                "last_update": last_update,
                "latest_link": latest_link,
            }
        else:
            try:
                text = await self.parse_channel_info(url)
                if text is None:
                    return message.plain_result(f"è¯·æ±‚é¢‘é“å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– URL: {url}")
                title, desc = self.data_handler.parse_channel_text_info(text)
                latest_item = await self.poll_rss(url)
            except Exception as e:
                return message.plain_result(f"è§£æé¢‘é“ä¿¡æ¯å¤±è´¥: {str(e)}")

            last_update = latest_item[-1].pubDate_timestamp if latest_item else int(time.time())
            latest_link = latest_item[-1].link if latest_item else ""

            self.data_handler.data[url] = {
                "subscribers": {
                    user: {
                        "cron_expr": cron_expr,
                        "last_update": last_update,
                        "latest_link": latest_link,
                    }
                },
                "info": {
                    "title": title,
                    "description": desc,
                },
            }
        self.data_handler.save_data()
        return self.data_handler.data[url]["info"]

    async def _get_chain_components(self, item: RSSItem):
        """ç»„è£…æ¶ˆæ¯é“¾ï¼ˆå·²ä¿®å¤æ—¶é—´ä¸æ’ç‰ˆï¼‰"""
        from datetime import datetime, timezone, timedelta
        
        comps = []
        text_parts = [] # ä½¿ç”¨åˆ—è¡¨æ”¶é›†æ‰€æœ‰æ–‡æœ¬ï¼Œæœ€åä¸€æ¬¡æ€§æ‹¼æ¥ï¼Œè§£å†³æ¢è¡Œä¸¢å¤±é—®é¢˜
        
        # 1. é†’ç›®çš„å¤´éƒ¨ï¼šé¢‘é“åç§°
        is_rt = title.upper().startswith("RT") or desc.upper().startswith("RT")
        source_name = item.chan_title.replace("Twitter @", "ğŸ¦ ").strip()
        text_parts.append(f"ğŸ“° ã€{item.chan_title}ã€‘")
        text_parts.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")# æ›¿æ¢ä¸ºè¾ƒè½»é‡çš„åˆ†å‰²çº¿
        
        # 4. ã€æ ¸å¿ƒä¿®å¤ã€‘å‹å¥½çš„å‘å¸ƒæ—¶é—´ (å¼ºåˆ¶ä½¿ç”¨ä¸œå…«åŒº UTC+8)
        # è®¾å®šä¸€ä¸ªå›ºå®šçš„ä¸œå…«åŒºæ—¶åŒºï¼Œç»•å¼€æœåŠ¡å™¨ç³»ç»Ÿçš„æœ¬åœ°æ—¶åŒºè®¾ç½®
        tz_utc_8 = timezone(timedelta(hours=8))
        formatted_time = ""
        
        # ä¼˜å…ˆè¯»å–åŸå§‹å­—ç¬¦ä¸²ï¼Œæ— è§†è¢«ä¸Šæ¸¸è§£æé”™è¯¯çš„æ—¶é—´æˆ³
        if getattr(item, 'pub_date', None):
            pub_date_str = str(item.pub_date).strip()
            
            try:
                # å°è¯• A: å¦‚æœå®ƒæ˜¯å®Œæ•´çš„ RSSHub æ ¼å¼ (å¦‚ Sat, 28 Feb 2026 06:37:12 GMT)
                dt = email.utils.parsedate_to_datetime(pub_date_str)
                formatted_time = dt.astimezone(tz_utc_8).strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                pass
                
            if not formatted_time:
                try:
                    # å°è¯• B: ä¸Šæ¸¸ççœ¼æŠŠå®ƒæˆªæ–­æˆäº†çº¯ç²¹çš„ "2026-02-28 06:37:12"
                    # æ ¹æ®ä½ çš„åé¦ˆï¼ŒRSSHubæºæ˜¯GMTï¼Œæ‰€ä»¥æˆ‘ä»¬å°†é”™å°±é”™ï¼Œå¼ºè¡Œç»™è¿™ä¸ªå­—ç¬¦ä¸²è´´ä¸Š UTC æ ‡ç­¾å¹¶ +8 å°æ—¶
                    dt = datetime.strptime(pub_date_str, "%Y-%m-%d %H:%M:%S")
                    dt = dt.replace(tzinfo=timezone.utc).astimezone(tz_utc_8)
                    formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                except Exception:
                    pass
                    
        # å°è¯• C: å¦‚æœå…¨å¤±è´¥äº†ï¼Œæ‰ä½¿ç”¨å¸¦æœ‰æ±¡æŸ“çš„ timestamp è¿›è¡Œæš´åŠ›ä¿®æ­£å…œåº•
        if not formatted_time and getattr(item, 'pubDate_timestamp', 0) > 0:
            try:
                # è·å–å®ƒé”™è¯¯çš„æœ¬åœ°å­—é¢æ—¶é—´ï¼Œå°†å…¶å¼ºè¡Œè§†ä¸º UTC æ—¶é—´ï¼Œç„¶åè½¬ä¸ºä¸œå…«åŒº
                dt_naive = datetime.fromtimestamp(item.pubDate_timestamp)
                dt_corrected = dt_naive.replace(tzinfo=timezone.utc).astimezone(tz_utc_8)
                formatted_time = dt_corrected.strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                pass

        # å…œåº•è¾“å‡ºæ—¶é—´
        if formatted_time:
            text_parts.append(f"ğŸ•’ {formatted_time}")
        elif getattr(item, 'pub_date', None):
            text_parts.append(f"ğŸ•’ {item.pub_date}")

        # 5. æ¥æºé“¾æ¥ (æ”¾ç½®åº•éƒ¨é˜²æ­¢æ’ç‰ˆè¢«ç ´å)
        if not getattr(self, 'is_hide_url', False) and getattr(item, 'link', None):
            text_parts.append(f"ğŸ”— {item.link}")

        text_parts.append("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        
        title = item.title.strip() if item.title else ""
        desc = item.description.strip() if item.description else ""

        display_desc = desc
        if is_rt:
            # å°† "RT " æ›¿æ¢ä¸ºå›¾æ ‡ï¼Œå¹¶å°è¯•æˆªæ–­è¿‡äºå†—é•¿çš„åšä¸»åç¼€ï¼ˆä»¥ç©ºæ ¼æˆ–ç‰¹å®šç¬¦å·åˆ†å‰²ï¼‰
            display_desc = display_desc.replace("RT ", "ğŸ”„ è½¬å‘è‡ª: ").replace("RTâ€‚", "ğŸ”„ è½¬å‘è‡ª: ")

        show_title = True
        if title and desc.startswith(title[:15]):
            show_title = False
        # 3. æ ‡é¢˜ä¸æ­£æ–‡é€»è¾‘å¤„ç†
        # å¦‚æœæ ‡é¢˜å­˜åœ¨ä¸”ä¸æ˜¯â€œæ— æ ‡é¢˜â€
        if show_title and title != "æ— æ ‡é¢˜" and not desc.startswith(title[:10]):
            text_parts.append(f"ğŸ“Œ {title}")
        if display_desc:
            # ç»™è¯é¢˜æ ‡ç­¾å‰åå¢åŠ ç©ºæ ¼ï¼Œæˆ–è€…å•ç‹¬æ¢è¡Œï¼ˆå¯é€‰ï¼‰
            display_desc = display_desc.replace("#", "\n#") 
            text_parts.append(f"ğŸ’¬ {display_desc}")

        # === åˆå¹¶æ‰€æœ‰æ–‡æœ¬ç»„ä»¶ ===
        # å°†ä¸Šé¢æ”¶é›†çš„æ‰€æœ‰æ–‡æœ¬ç”¨æ¢è¡Œç¬¦(\n)è¿æ¥æˆä¸€ä¸ªå®Œæ•´çš„å­—ç¬¦ä¸²
        # å½»åº•é¿å…å¤šä¸ª Comp.Plain æ‹¼æ¥æ—¶å¯¼è‡´çš„æ’ç‰ˆæŒ¤å‹å’Œæ¢è¡Œå¤±æ•ˆé—®é¢˜
        full_text = "\n".join(text_parts)
        comps.append(Comp.Plain(full_text))

        # 7. å›¾ç‰‡å¤„ç† (ä¿ç•™åŸé€»è¾‘)
        if self.is_read_pic and item.pic_urls:
            temp_max_pic_item = len(item.pic_urls) if self.max_pic_item == -1 else self.max_pic_item
            for pic_url in item.pic_urls[:temp_max_pic_item]:
                base64str = await self.pic_handler.modify_corner_pixel_to_base64(pic_url)
                if base64str is None:
                    comps.append(Comp.Plain("\n[å›¾ç‰‡åŠ è½½å¤±è´¥]"))
                    continue
                else:
                    comps.append(Comp.Plain("\n")) # æ¢è¡Œé˜²ç²˜è¿
                    comps.append(Comp.Image.fromBase64(base64str))
                    
        return comps
    
    def _is_url_or_ip(self,text: str) -> bool:
        """
        åˆ¤æ–­ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ä¸ºç½‘å€ï¼ˆhttp/https å¼€å¤´ï¼‰æˆ– IP åœ°å€ã€‚
        """
        url_pattern = r"^(?:http|https)://.+$"
        ip_pattern = r"^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(:\d+)?$"
        return bool(re.match(url_pattern, text) or re.match(ip_pattern, text))

    @filter.command_group("rss", alias={"RSS"})
    def rss(self):
        pass

    @rss.group("rsshub")
    def rsshub(self, event: AstrMessageEvent):
        pass

    @rsshub.command("add")
    async def rsshub_add(self, event: AstrMessageEvent, url: str):
        if url.endswith("/"):
            url = url[:-1]
        url = self.parse_rss_url(url)
        
        if not self._is_url_or_ip(url):
            yield event.plain_result("è¯·è¾“å…¥æ­£ç¡®çš„URL")
            return
        elif url in self.data_handler.data["rsshub_endpoints"]:
            yield event.plain_result("è¯¥RSSHubç«¯ç‚¹å·²å­˜åœ¨")
            return
        else:
            self.data_handler.data["rsshub_endpoints"].append(url)
            self.data_handler.save_data()
            yield event.plain_result("æ·»åŠ æˆåŠŸ")

    @rsshub.command("list")
    async def rsshub_list(self, event: AstrMessageEvent):
        ret = "å½“å‰Botæ·»åŠ çš„rsshub endpointï¼š\n"
        yield event.plain_result(
            ret
            + "\n".join(
                [
                    f"{i}: {x}"
                    for i, x in enumerate(self.data_handler.data["rsshub_endpoints"])
                ]
            )
        )

    @rsshub.command("remove")
    async def rsshub_remove(self, event: AstrMessageEvent, idx: int):
        if idx < 0 or idx >= len(self.data_handler.data["rsshub_endpoints"]):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ")
            return
        else:
            self.data_handler.data["rsshub_endpoints"].pop(idx)
            self.data_handler.save_data()
            self._fresh_asyncIOScheduler()
            yield event.plain_result("åˆ é™¤æˆåŠŸ")

    @rss.command("add")
    async def add_command(
        self,
        event: AstrMessageEvent,
        idx: int,
        route: str,
        minute: str,
        hour: str,
        day: str,
        month: str,
        day_of_week: str,
    ):
        if idx < 0 or idx >= len(self.data_handler.data["rsshub_endpoints"]):
            yield event.plain_result(
                "ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss rsshub list æŸ¥çœ‹å·²ç»æ·»åŠ çš„ rsshub endpoint"
            )
            return
        if not route.startswith("/"):
            yield event.plain_result("è·¯ç”±å¿…é¡»ä»¥ / å¼€å¤´")
            return

        url = self.data_handler.data["rsshub_endpoints"][idx] + route
        cron_expr = f"{minute} {hour} {day} {month} {day_of_week}"

        ret = await self._add_url(url, cron_expr, event)
        if isinstance(ret, MessageEventResult):
            yield ret
            return
        else:
            chan_title = ret["title"]
            chan_desc = ret["description"]

        self._fresh_asyncIOScheduler()

        yield event.plain_result(
            f"æ·»åŠ æˆåŠŸã€‚é¢‘é“ä¿¡æ¯ï¼š\næ ‡é¢˜: {chan_title}\næè¿°: {chan_desc}"
        )

    @rss.command("add-url")
    async def add_url_command(
        self,
        event: AstrMessageEvent,
        url: str,
        minute: str,
        hour: str,
        day: str,
        month: str,
        day_of_week: str,
    ):
        url = self.parse_rss_url(url)
        cron_expr = f"{minute} {hour} {day} {month} {day_of_week}"
        ret = await self._add_url(url, cron_expr, event)
        if isinstance(ret, MessageEventResult):
            yield ret
            return
        else:
            chan_title = ret["title"]
            chan_desc = ret["description"]

        self._fresh_asyncIOScheduler()

        yield event.plain_result(
            f"æ·»åŠ æˆåŠŸã€‚é¢‘é“ä¿¡æ¯ï¼š\næ ‡é¢˜: {chan_title}\næè¿°: {chan_desc}"
        )
        

    @rss.command("list")
    async def list_command(self, event: AstrMessageEvent):
        user = event.unified_msg_origin
        subs_urls = self.data_handler.get_subs_channel_url(user)
        if not subs_urls:
            yield event.plain_result("å½“å‰æ²¡æœ‰ä»»ä½•è®¢é˜…ã€‚")
            return
        ret = "ğŸ“‹ å½“å‰è®¢é˜…åˆ—è¡¨ï¼š\n"
        for i, url in enumerate(subs_urls):
            info = self.data_handler.data[url]["info"]
            cron = self.data_handler.data[url]["subscribers"][user].get("cron_expr", "* * * * *")
            ret += f"{i}. ã€{info['title']}ã€‘\n   å‘¨æœŸ: `{cron}`\n"
        yield event.plain_result(ret.strip())

    @rss.command("remove")
    async def remove_command(self, event: AstrMessageEvent, idx: int):
        subs_urls = self.data_handler.get_subs_channel_url(event.unified_msg_origin)
        if idx < 0 or idx >= len(subs_urls):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss list æŸ¥çœ‹å·²ç»æ·»åŠ çš„è®¢é˜…")
            return
        url = subs_urls[idx]
        self.data_handler.data[url]["subscribers"].pop(event.unified_msg_origin)

        if not self.data_handler.data[url]["subscribers"]:
            self.data_handler.data.pop(url)

        self.data_handler.save_data()

        self._fresh_asyncIOScheduler()
        yield event.plain_result("åˆ é™¤æˆåŠŸ")

    @rss.command("update")
    async def update_command(self, event: AstrMessageEvent, idx: int, minute: str, hour: str, day: str, month: str, day_of_week: str):
        user = event.unified_msg_origin
        subs_urls = self.data_handler.get_subs_channel_url(user)
        if idx < 0 or idx >= len(subs_urls):
            yield event.plain_result("ç´¢å¼•é”™è¯¯ã€‚")
            return
        url = subs_urls[idx]
        new_cron = f"{minute} {hour} {day} {month} {day_of_week}"
        self.data_handler.data[url]["subscribers"][user]["cron_expr"] = new_cron
        self.data_handler.save_data()
        self._fresh_asyncIOScheduler()
        yield event.plain_result(f"âœ… æ›´æ–°æˆåŠŸï¼\né¢‘é“: {self.data_handler.data[url]['info']['title']}\næ–°å‘¨æœŸ: `{new_cron}`")

    @rss.command("get")
    async def get_command(self, event: AstrMessageEvent, idx: int):
        subs_urls = self.data_handler.get_subs_channel_url(event.unified_msg_origin)
        if idx < 0 or idx >= len(subs_urls):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss list æŸ¥çœ‹å·²ç»æ·»åŠ çš„è®¢é˜…")
            return
        url = subs_urls[idx]
        rss_items = await self.poll_rss(url)
        if not rss_items:
            yield event.plain_result("æ²¡æœ‰æ–°çš„è®¢é˜…å†…å®¹")
            return
        
        item = rss_items[-1]
        user_parts = event.unified_msg_origin.split(":")
        platform_name = user_parts[0] if len(user_parts) > 0 else ""

        comps = await self._get_chain_components(item)
        if(platform_name == "aiocqhttp" and self.is_compose):
            node = Comp.Node(
                    uin=0,
                    name="Astrbot",
                    content=comps
                )
            yield event.chain_result([node]).use_t2i(self.t2i)
        else:
            yield event.chain_result(comps).use_t2i(self.t2i)